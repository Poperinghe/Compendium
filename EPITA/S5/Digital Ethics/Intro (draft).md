The question of digital ethics is asked whenever autonomous systems come into play.

The class will be split into 4 themes:
1) Autonomous machines (driver-less cars)
2) Predictive justice
3) Recommendation Algorithms
4) Digital Ethics in the age of controversy

Ethics = branch of philosophy outlining the fundamental principles of human conducts. Linked but not equal to morality.

### Ethics vs Morality

Ethics outlines what a person "should" do.
Morality tells you what you "have" to do (idea of obligation). -> based on cultural or religious environment.

Morality guides individual and collective behavior whereas ethics is only relevant to an individual (often the "self").

Ethics is adaptable morality is fixed. It evolves along with society and philosophy.

### Ethical Issues in Computing

Data Ethics: All of your medical data (which should be private) is used to train artificial intelligence. Is it okay to use people's private information, and therefore make it public, if the reason is to potentially help research efforts.

AI Ethics: Artificial intelligence tools making predictions (like on a persons health for example) without a traceable explanation -> issue of transparency / the black box -> i might have cancer and an AI tells me i have cancer, there is not only the issue that the AI gave me cancer (Schrodinger's cat), but also another layer of the issue is that there is no backtrace (we cant know why it said that so we don't know if its definitely wrong or right).

Ethics of Pervasive Computing: ?
...

### New Questions with AI

Will AI spell the end of the human race? Stephen Hawking asks the question in 2014 and changes his mind later. The main problematic is: where can we apply AI? Because there are clearly some places where it is beneficial and safe. The 23 Asilomar AI principles ratified by Stephen Hawking and Elon Musk outline this. 

Responsibility. If i ask ChatGPT to write some code and it sucks and causes a problem, who is responsible? Me or the computer?

Transparency: users of a service should be able to know if information that they have been given was generated by an AI or not.

AI widens the economic gap between different populations. Everyone should be able to benefit from it.

Can autonomous machines decide to kill us?

### History of AI

Isaac Asimov and his three laws of robotics (1942):

1) The First Law: A robot may not injure a human being or, through inaction, allow a human being to come to harm.

2) The Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

3) The Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

However now Asimov's ideas on "Robots" have become obsolete. Machines are now becoming weapons -> his first law is no longer being respected. "Robots" can also break laws without knowing it. Their ability to respect the laws are almost entirely dependent on how well the engineers built it (and will therefore never be perfect). Although non deterministic behavior is not necessarily an issue from an engineering point of view -> it is a huge problem in terms of Ethics and for Johnny Law.

IEEE = Institute of Electrical and Electronics Engineers (can be trusted apparently)

### Which Machines are Concerned by Ethics?

Example (this one is weird): In Qatar there are camel races. To find good jockeys for the races you need to find people who are light -> child jockeys -> children don't want to be jockeys -> child slavery.
Solution: replace children with machines -> no more child slavery...
*(sure... i guess...)*

Lethal Autonomous Weapons: ...

Autonomous Vehicles: In the case of a crash, should you save the driver or the other person?

### Other Notions

Deontology = The study of the obligations of people based on morality. Mainly useful in medicine. This principle is one of the rare notions of philosophy that is actually clearly outlined somewhere (the french medical code of deontology).

Emanuel Kant's principles of ethics: only the intention of your actions matters when asking whether what you did was moral.

Consequentialism from Gertrude Elizabeth Margaret. directly opposes Kant by saying that you should only be judged on the consequences of your actions at different time scales (short, medium, and long term) -> the ends can always justify the means.

Utilitarianism = Maximizing the happiness of the greatest number (even if it means killing someone for example).

The trolley problem = there is a train that is going to kill 5 people if you do nothing. You have the option to divert the tracks so that the train kills only 1 (other) person. The question being: do you divert the tracks? The answer seems simple, divert the tracks and you save 4 lives. However the problem is your action. If the 5 people die they were going to die anyways, you had would have had no part in their death but if you divert the tracks you are killing the 1 person by your action. Do you want to be responsible for that?
